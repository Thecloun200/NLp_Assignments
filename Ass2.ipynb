{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "63ZCVyKHcvlT",
        "outputId": "2372a26f-eaf7-4184-9ddb-d6824238f1b7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install transformers\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-z87ZslYXmmr",
        "outputId": "175eacbc-9dec-451a-98cc-d3fee8e244db"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.38.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.13.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.19.3 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.20.3)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.25.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (24.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2023.12.25)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.31.0)\n",
            "Requirement already satisfied: tokenizers<0.19,>=0.14 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.15.2)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.2)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.2)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.19.3->transformers) (2023.6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.19.3->transformers) (4.10.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2024.2.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import pipeline\n"
      ],
      "metadata": {
        "id": "3y6H41CjXpg4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CdD0mzmgXFNc"
      },
      "outputs": [],
      "source": [
        "import random\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk import pos_tag\n",
        "import string\n",
        "import re\n",
        "from nltk.corpus import wordnet\n",
        "import numpy as np\n",
        "import math\n",
        "import string\n",
        "from collections import Counter\n",
        "from scipy.sparse import csr_matrix\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import string\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "def preprocess_text(text):\n",
        "    # Remove punctuation\n",
        "    preprocessed_data = text.translate(str.maketrans('', '', string.punctuation))\n",
        "    # Convert to lower case\n",
        "    preprocessed_data = preprocessed_data.lower()\n",
        "    # Tokenization\n",
        "    tokens = word_tokenize(preprocessed_data)\n",
        "    # Remove stop words\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    tokens = [word for word in tokens if word not in stop_words]\n",
        "    # Lemmatization\n",
        "    lemmatizer = WordNetLemmatizer()\n",
        "    tokens = [lemmatizer.lemmatize(word) for word in tokens]\n",
        "    return tokens\n",
        "\n",
        "\n",
        "# Function to generate a random sentence using GPT-2 model\n",
        "def generate_random_sentence(prompt,length):\n",
        "    generator = pipeline('text-generation', model='gpt2')\n",
        "    generated_text = generator(prompt, max_length=50)[0]['generated_text']\n",
        "    return generated_text\n",
        "\n",
        "\n",
        "\n",
        "def generate_preprocessed_document( prompt,length):\n",
        "    document =generate_random_sentence(prompt,length)\n",
        "    return preprocess_text(document)"
      ],
      "metadata": {
        "id": "ZS1FEjz2YV7j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "length = 100\n",
        "prompt1 = \"Artificial intelligence (AI) is\"\n",
        "prompt2 = \"Data science is\"\n",
        "preprocessed_document1 = generate_preprocessed_document( prompt1 , length)\n",
        "preprocessed_document2= generate_preprocessed_document(prompt2 , length)\n"
      ],
      "metadata": {
        "id": "KHI2q0QbYV95",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d6f4d354-3879-4a5b-d990-0cb7990dbf05"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(preprocessed_document1)\n",
        "print(len(preprocessed_document1))\n",
        "unique_words_document1 = set(preprocessed_document1)\n",
        "unique_words_document2 = set(preprocessed_document2)\n",
        "print(\"Unique words:\", unique_words_document1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z3cR9484dsJj",
        "outputId": "b5ea1787-07bf-4064-c3ad-d51f338b6ac6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['artificial', 'intelligence', 'ai', 'becoming', 'driving', 'factor', 'generation', 'technology', 'could', 'someday', 'drive', 'robot', 'point', 'job', 'job', 'food', 'delivery', 'even', 'company', 'computer', 'based', 'artificial', 'intelligence', 'could', 'become']\n",
            "25\n",
            "Unique words: {'delivery', 'becoming', 'factor', 'even', 'ai', 'drive', 'robot', 'someday', 'become', 'food', 'intelligence', 'could', 'generation', 'company', 'driving', 'technology', 'point', 'computer', 'artificial', 'based', 'job'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def term_frequency(doc):\n",
        "    tf = {}\n",
        "    word_counts = Counter(doc)\n",
        "    total_words = len(doc)\n",
        "    for word, count in word_counts.items():\n",
        "        tf[word] = count / total_words\n",
        "    return tf\n",
        "\n",
        "\n",
        "import math\n",
        "import builtins\n",
        "\n",
        "import math\n",
        "\n",
        "def inverse_document_frequency(documents):\n",
        "    idf = {}\n",
        "    total_documents = len(documents)\n",
        "    all_words = set([word for doc in documents for word in doc])\n",
        "    for word in all_words:\n",
        "        document_count = sum([1 for doc in documents if word in doc])\n",
        "        idf[word] = math.log(total_documents / (1 + document_count))+1\n",
        "    return idf\n",
        "\n",
        "\n",
        "def tfidf(documents):\n",
        "    tfidf_matrix = []\n",
        "    idf = inverse_document_frequency(documents)\n",
        "    for doc in documents:\n",
        "        tf = term_frequency(doc)\n",
        "        tfidf_vector = {}\n",
        "        for word in tf:\n",
        "            tfidf_vector[word] = tf[word] * idf[word]\n",
        "        tfidf_matrix.append(tfidf_vector)\n",
        "    return tfidf_matrix\n",
        "\n",
        "\n",
        "\n",
        "def print_tfIdf(tfidf_matrix, documents):\n",
        "    for i, doc_tfidf in enumerate(tfidf_matrix):\n",
        "        print(f\"Document {i + 1}:\")\n",
        "        for word in documents[i].split():\n",
        "            tfidf_value = doc_tfidf.get(word, 0)  # Get TF-IDF value for the word, default to 0 if word not found\n",
        "            print(f\"{word}: {tfidf_value}\")\n",
        "\n"
      ],
      "metadata": {
        "id": "W1Xzj1tNg7KC"
      },
      "execution_count": 60,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "all_preprocessed_docs = [' '.join(preprocessed_document1), ' '.join(preprocessed_document2)]\n",
        "print(term_frequency(preprocessed_document2))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m_sDSBHPhBoq",
        "outputId": "832dcc86-23e6-4e83-d33b-3aae7763208f"
      },
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'data': 0.037037037037037035, 'science': 0.037037037037037035, 'great': 0.07407407407407407, 'way': 0.037037037037037035, 'think': 0.037037037037037035, 'world': 0.037037037037037035, 'without': 0.037037037037037035, 'making': 0.037037037037037035, 'mistake': 0.037037037037037035, 'focusing': 0.037037037037037035, 'much': 0.037037037037037035, 'time': 0.037037037037037035, 'pesky': 0.037037037037037035, 'thing': 0.037037037037037035, 'possible': 0.037037037037037035, 'order': 0.037037037037037035, 'make': 0.037037037037037035, 'quick': 0.037037037037037035, 'trip': 0.037037037037037035, 'video': 0.07407407407407407, 'youll': 0.037037037037037035, 'need': 0.037037037037037035, 'create': 0.037037037037037035, 'simple': 0.037037037037037035, 'app': 0.037037037037037035}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tfidf(all_preprocessed_docs)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qgtL-tklanUA",
        "outputId": "9e9e20f4-619d-4565-f368-73b0c5612b8a"
      },
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'a': 0.031127481250881446,\n",
              "  'r': 0.028014733125793303,\n",
              "  't': 0.031127481250881446,\n",
              "  'i': 0.056029466251586606,\n",
              "  'f': 0.01245099250035258,\n",
              "  'c': 0.03735297750105773,\n",
              "  'l': 0.031127481250881446,\n",
              "  ' ': 0.07470595500211547,\n",
              "  'n': 0.03735297750105773,\n",
              "  'e': 0.06225496250176289,\n",
              "  'g': 0.018676488750528866,\n",
              "  'b': 0.018676488750528866,\n",
              "  'o': 0.056029466251586606,\n",
              "  'm': 0.015563740625440723,\n",
              "  'd': 0.02490198500070516,\n",
              "  'v': 0.01245099250035258,\n",
              "  'h': 0.003112748125088145,\n",
              "  'y': 0.01245099250035258,\n",
              "  'u': 0.009338244375264433,\n",
              "  's': 0.00622549625017629,\n",
              "  'p': 0.009338244375264433,\n",
              "  'j': 0.010471204188481676},\n",
              " {'d': 0.021489212959945863,\n",
              "  'a': 0.03581535493324311,\n",
              "  't': 0.039396890426567424,\n",
              "  ' ': 0.09311992282643208,\n",
              "  's': 0.025070748453270175,\n",
              "  'c': 0.021489212959945863,\n",
              "  'i': 0.05014149690654035,\n",
              "  'e': 0.060886103386513284,\n",
              "  'n': 0.021489212959945863,\n",
              "  'g': 0.017907677466621554,\n",
              "  'r': 0.025070748453270175,\n",
              "  'w': 0.018072289156626505,\n",
              "  'y': 0.010744606479972931,\n",
              "  'h': 0.014326141973297244,\n",
              "  'k': 0.03614457831325301,\n",
              "  'o': 0.028652283946594487,\n",
              "  'l': 0.017907677466621554,\n",
              "  'u': 0.017907677466621554,\n",
              "  'm': 0.021489212959945863,\n",
              "  'f': 0.003581535493324311,\n",
              "  'p': 0.021489212959945863,\n",
              "  'b': 0.003581535493324311,\n",
              "  'q': 0.006024096385542169,\n",
              "  'v': 0.007163070986648622}]"
            ]
          },
          "metadata": {},
          "execution_count": 62
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "inverse_document_frequency(all_preprocessed_docs)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9p129nG2mQOy",
        "outputId": "9cdb2d48-c168-440d-f93b-4b4aa3c3994d"
      },
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'n': 0.5945348918918356,\n",
              " 'q': 1.0,\n",
              " 'b': 0.5945348918918356,\n",
              " 'p': 0.5945348918918356,\n",
              " 'a': 0.5945348918918356,\n",
              " 'e': 0.5945348918918356,\n",
              " 'i': 0.5945348918918356,\n",
              " 'm': 0.5945348918918356,\n",
              " 'r': 0.5945348918918356,\n",
              " 'j': 1.0,\n",
              " ' ': 0.5945348918918356,\n",
              " 'l': 0.5945348918918356,\n",
              " 'h': 0.5945348918918356,\n",
              " 'f': 0.5945348918918356,\n",
              " 'g': 0.5945348918918356,\n",
              " 'k': 1.0,\n",
              " 'o': 0.5945348918918356,\n",
              " 'v': 0.5945348918918356,\n",
              " 'd': 0.5945348918918356,\n",
              " 'w': 1.0,\n",
              " 'y': 0.5945348918918356,\n",
              " 't': 0.5945348918918356,\n",
              " 'u': 0.5945348918918356,\n",
              " 's': 0.5945348918918356,\n",
              " 'c': 0.5945348918918356}"
            ]
          },
          "metadata": {},
          "execution_count": 63
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# TFIDF calculation\n",
        "tfidf_vectorizer = TfidfVectorizer()\n",
        "tfidf_matrix = tfidf_vectorizer.fit_transform(all_preprocessed_docs)\n",
        "\n",
        "# Get feature names\n",
        "feature_names = tfidf_vectorizer.get_feature_names_out()\n",
        "\n",
        "# Get TF for each word for all documents\n",
        "tf_matrix = tfidf_matrix.toarray()\n",
        "print(\"\\nTF Matrix:\")\n",
        "print(tf_matrix)\n",
        "\n",
        "# Get IDF for each word\n",
        "idf = tfidf_vectorizer.idf_\n",
        "print(\"\\nIDF:\")\n",
        "print(idf)\n",
        "\n",
        "# Multiply TF * IDF\n",
        "tfidf = tf_matrix * idf\n",
        "print(\"\\nTFIDF Matrix:\")\n",
        "print(tfidf)\n",
        "\n",
        "# Get normalized TFIDF\n",
        "normalized_tfidf = tfidf / tfidf.max(axis=1).reshape(-1, 1)\n",
        "print(\"Normalized TFIDF:\")\n",
        "print(normalized_tfidf)"
      ],
      "metadata": {
        "id": "ltuBM-thsC-5"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}